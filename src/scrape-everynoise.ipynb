{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everynoise.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This version: 6 July 2018*\n",
    "*Updated on 8 April 2019*\n",
    "\n",
    "Comments: h.datta@tilburguniversity.edu\n",
    "\n",
    "**Requires Python 3.x**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "Installs / loads a bunch of packages required to conduct web scraping. Run this before you go to the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and install necessary packages\n",
    "import lxml\n",
    "import selenium\n",
    "import pandas\n",
    "import bs4\n",
    "\n",
    "# Load packages into memory\n",
    "import urllib3\n",
    "import datetime\n",
    "from lxml import etree \n",
    "import time\n",
    "import codecs\n",
    "import io\n",
    "from lxml.cssselect import CSSSelector\n",
    "from lxml.etree import fromstring\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Function to create a new directory if it does not exist yet\n",
    "def makedir(dirname):\n",
    "    try:\n",
    "        os.stat(dirname)\n",
    "    except:\n",
    "        os.mkdir(dirname)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a few functions\n",
    "def load_page(url):\n",
    "    request=urllib.request.urlopen(url)\n",
    "    request.headers.getparam('charset')\n",
    "    encoding='UTF-8'\n",
    "    content=request.read().decode(encoding)\n",
    "    return(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect all available dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-6286db6cb5f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load page\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http://everynoise.com/sorting_hat_closet/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# extract dates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0metree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0metree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTMLParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-27dcb8907e3e>\u001b[0m in \u001b[0;36mload_page\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# a few functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetparam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'charset'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'UTF-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             response = self.parent.error(\n\u001b[1;32m--> 641\u001b[1;33m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "# load page\n",
    "page=load_page('http://everynoise.com/sorting_hat_closet/')\n",
    "\n",
    "# extract dates\n",
    "tree = etree.fromstring(page, parser=etree.HTMLParser())\n",
    "\n",
    "elements=tree.xpath('/html/body/a')\n",
    "dates=[]\n",
    "for el in elements:\n",
    "    dates.append(el.text)\n",
    "print(\"Found \" + str(len(dates)) + \" dates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all pages\n",
    "makedir('raw')\n",
    "\n",
    "for date in dates:\n",
    "    url='http://everynoise.com/sorting_hat_closet/spotify_new_releases_'+ date+'.html'\n",
    "    print('downloading ' + url + '...')\n",
    "    content=load_page(url)\n",
    "    f=io.open('raw/'+date+'.html','w',encoding='UTF-8')\n",
    "    f.write(content)\n",
    "    f.close()\n",
    "    print('...sleeping 10 sec...')\n",
    "    time.sleep(10)\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to parse content from release website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract release IDs\n",
    "def get_albumid(item):\n",
    "    \n",
    "    albumid=item.find('div', attrs={\"class\" : \"albumwrapper play\"}).get('albumid')\n",
    "    return(albumid)\n",
    "\n",
    "# extract all release IDs in a genre section\n",
    "def get_albums_from_genre(genresection):\n",
    "    items=genresection.findAll(\"div\", {'class': re.compile(\"albumbox\")})\n",
    "    out = []\n",
    "    for i in items:\n",
    "        out.append(get_albumid(i))\n",
    "    return(out)\n",
    "\n",
    "# parses album info from right side of screen\n",
    "def get_album_info(item):\n",
    "\n",
    "    try:\n",
    "        albumid=item.find('span', attrs={\"class\" : \"play trackcount\"}).get('albumid')\n",
    "    except:\n",
    "        albumid=item.find('a', href=True)\n",
    "\n",
    "    artistname=item.find('span', attrs={\"class\" : \"creditartist\"}).string\n",
    "    try:\n",
    "        releasename=item.find('span', attrs={\"class\" : \"creditrelease\"}).string\n",
    "        if releasename is None: releasename= item.find('span', attrs={\"class\" : \"creditrelease\"}).encode('UTF-8')\n",
    "    except:\n",
    "        releasename = '' \n",
    "        \n",
    "    try:\n",
    "        artistrank=item.get('title').replace('artist rank: ','')\n",
    "    except:\n",
    "        artistrank = 'NA'\n",
    "    try:\n",
    "        playtrackcount=item.find('span', attrs={\"class\" : \"play trackcount\"}).string\n",
    "    except:\n",
    "        try:\n",
    "            playtrackcount=item.find('span', attrs={\"class\" : \"play trackcount singletrack\"}).string\n",
    "        except:\n",
    "            playtrackcount = 'NA'\n",
    "    classname='-'.join(item.get('class'))\n",
    "    \n",
    "    return([artistname,releasename,albumid,artistrank,playtrackcount,classname])\n",
    "\n",
    "\n",
    "# find all genre sections and save album IDs and relevant genres\n",
    "def get_all_genres(soup):\n",
    "    genres = soup.findAll(\"div\", {\"class\": \"leftgenre genre\"})\n",
    "\n",
    "    genredata=[]\n",
    "\n",
    "    for genre in genres:\n",
    "        genre.find(\"span\", {'class': 'number'}).string\n",
    "        genr_string = \"\"\n",
    "        for genr in genre.findAll(\"a\", {\"class\": \"clustergenre\"}):\n",
    "            genr_string=genr_string+genr.string+'|'\n",
    "        #print(genr_string)\n",
    "        #print(ct)\n",
    "        #ct=ct+1\n",
    "        content = get_albums_from_genre(genre)\n",
    "\n",
    "        for conn in content:\n",
    "            genredata.append([conn, genr_string])\n",
    "\n",
    "    genr=pandas.DataFrame(genredata)\n",
    "    genr.rename(columns={0: 'albumid', 1: 'genre'}, inplace=True)\n",
    "    return(genr)\n",
    "\n",
    "# gets all releases from right sight of the screen\n",
    "def get_all_releases(soup):\n",
    "    #allreleases = soup.find(\"td\", {\"class\": \"allreleases\"})\n",
    "    unfiltered_items=soup.findAll(\"div\", {'class': re.compile(\"album|other\")})\n",
    "    # loop to identify correct elements\n",
    "    items=[]\n",
    "    for i in unfiltered_items:\n",
    "        if 'class=\"other \"' in i.encode('UTF-8'): items.append(i)\n",
    "        if 'class=\"other single\"' in i.encode('UTF-8'): items.append(i)\n",
    "        if 'class=\"album \"' in i.encode('UTF-8'): items.append(i)\n",
    "        if 'class=\"album single\"' in i.encode('UTF-8'): items.append(i)\n",
    "    \n",
    "    out = []\n",
    "    c=0\n",
    "    for i in items:\n",
    "        c=c+1\n",
    "        out.append(get_album_info(i))\n",
    "    releases=pandas.DataFrame(out)\n",
    "    releases.rename(columns={0: 'artistname', 1: 'albumname', 2: 'albumid', 3:'artistrank', 4: 'playcountrank', 5: 'releasetype'}, inplace=True)\n",
    "\n",
    "    return(releases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to process all data: takes downloaded raw html file as input\n",
    "def process(fn='c:/Users/hanne/Dropbox/Tilburg/Projects/NewReleases/data/raw/2018-06-29.html'):\n",
    "    print('processing: '+fn)\n",
    "    # read file\n",
    "    g=io.open(fn,'r', encoding='UTF-8')\n",
    "    # turn into soup\n",
    "    soup = BeautifulSoup(g.read(), 'html.parser')\n",
    "    g.close()\n",
    "    # extract date (used to store in table and file name)\n",
    "    date=soup.find('div', attrs={\"class\" : \"title\"}).string[-10:]\n",
    "    print('getting genres')\n",
    "    genres=get_all_genres(soup)\n",
    "    print('getting all releases')\n",
    "    releases=get_all_releases(soup)\n",
    "    # merging genres to all releases\n",
    "    print('merging')\n",
    "    df2=releases.merge(genres, how='left', on='albumid')\n",
    "    # saving\n",
    "    df2['date'] = date\n",
    "    df2.to_csv('out/spotify-releases_'+ date+ '.csv', encoding= 'UTF-8',sep='\\t', index=False)\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: c:/Users/hanne/Dropbox/Tilburg/Projects/NewReleases/data/raw/2018-06-29.html\n",
      "getting genres\n",
      "getting all releases\n",
      "merging\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# try out for one date\n",
    "process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect files\n",
    "mypath='raw/'\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "for fn in onlyfiles:\n",
    "    process(fn='raw/'+fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
